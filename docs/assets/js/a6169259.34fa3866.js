"use strict";(self.webpackChunkstreamsheets=self.webpackChunkstreamsheets||[]).push([[94720],{83166:(e,t,n)=>{var o=n(64836);t.Z=void 0;var a=o(n(64938)),r=n(85893),i=(0,a.default)((0,r.jsx)("path",{d:"M12 2 4 5v6.09c0 5.05 3.41 9.76 8 10.91 4.59-1.15 8-5.86 8-10.91V5l-8-3zm-1.06 13.54L7.4 12l1.41-1.41 2.12 2.12 4.24-4.24 1.41 1.41-5.64 5.66z"}),"GppGood");t.Z=i},87382:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>k,contentTitle:()=>p,default:()=>b,frontMatter:()=>h,metadata:()=>g,toc:()=>f});var o=n(3905),a=n(49044),r=Object.defineProperty,i=Object.defineProperties,l=Object.getOwnPropertyDescriptors,s=Object.getOwnPropertySymbols,d=Object.prototype.hasOwnProperty,c=Object.prototype.propertyIsEnumerable,u=(e,t,n)=>t in e?r(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,m=(e,t)=>{for(var n in t||(t={}))d.call(t,n)&&u(e,n,t[n]);if(s)for(var n of s(t))c.call(t,n)&&u(e,n,t[n]);return e};const h={id:"mosquitto-high-availability",title:"Mosquitto High Availability Cluster",sidebar_label:"High Availability Cluster"},p=void 0,g={unversionedId:"Leftovers/mosquitto-high-availability",id:"version-3.0/Leftovers/mosquitto-high-availability",title:"Mosquitto High Availability Cluster",description:"Introduction",source:"@site/mosquitto_versioned_docs/version-3.0/Leftovers/ha-cluster.md",sourceDirName:"Leftovers",slug:"/Leftovers/mosquitto-high-availability",permalink:"/mosquitto/Leftovers/mosquitto-high-availability",draft:!1,tags:[],version:"3.0",frontMatter:{id:"mosquitto-high-availability",title:"Mosquitto High Availability Cluster",sidebar_label:"High Availability Cluster"}},k={},f=[{value:"Introduction",id:"introduction",level:2},{value:"Cluster Modes",id:"cluster-modes",level:2},{value:"Cluster architecture",id:"cluster-architecture",level:2},{value:"Overview",id:"overview",level:3},{value:"Docker image installation",id:"docker-image-installation",level:3},{value:"Broker Configuration",id:"broker-configuration",level:3},{value:"Load balancer configuration",id:"load-balancer-configuration",level:3},{value:"Full Sync HA Proxy Configuration",id:"full-sync-ha-proxy-configuration",level:4},{value:"Dynsec Sync HA Proxy Configuration",id:"dynsec-sync-ha-proxy-configuration",level:4},{value:"Control API for Cluster",id:"control-api-for-cluster",level:2},{value:"Creating the Cluster",id:"creating-the-cluster",level:3},{value:"Configuring the first node",id:"configuring-the-first-node",level:4},{value:"Joining the nodes to the cluster",id:"joining-the-nodes-to-the-cluster",level:4},{value:"Adding further nodes",id:"adding-further-nodes",level:3},{value:"Informing the cluster",id:"informing-the-cluster",level:4},{value:"Joining the node to the cluster",id:"joining-the-node-to-the-cluster",level:4},{value:"Removing nodes from Cluster",id:"removing-nodes-from-cluster",level:3},{value:"Inform Cluster Leader",id:"inform-cluster-leader",level:3},{value:"Leave Cluster",id:"leave-cluster",level:4},{value:"Deleting the cluster",id:"deleting-the-cluster",level:3},{value:"Getting cluster information",id:"getting-cluster-information",level:3},{value:"Getting node information",id:"getting-node-information",level:3},{value:"Set the cluster leader",id:"set-the-cluster-leader",level:3},{value:"Overview of Monitoring Methods",id:"overview-of-monitoring-methods",level:2},{value:"1. <strong>System Topics</strong>",id:"1-system-topics",level:3},{value:"2. <strong>Metrics Plugin</strong>",id:"2-metrics-plugin",level:3},{value:"3. <strong>Audit Trail Plugin</strong>",id:"3-audit-trail-plugin",level:3},{value:"Cluster System Requirements",id:"cluster-system-requirements",level:2},{value:"Server hardware",id:"server-hardware",level:3},{value:"Network",id:"network",level:3}],v={toc:f};function b(e){var t,r=e,{components:u}=r,h=((e,t)=>{var n={};for(var o in e)d.call(e,o)&&t.indexOf(o)<0&&(n[o]=e[o]);if(null!=e&&s)for(var o of s(e))t.indexOf(o)<0&&c.call(e,o)&&(n[o]=e[o]);return n})(r,["components"]);return(0,o.kt)("wrapper",(t=m(m({},v),h),i(t,l({components:u,mdxType:"MDXLayout"}))),(0,o.kt)(a.Z,{mdxType:"PremiumFeature"}),(0,o.kt)("h2",m({},{id:"introduction"}),"Introduction"),(0,o.kt)("p",null,"The Cedalo high availability feature provides the ability to run a cluster of Mosquitto nodes with a single leader and multiple followers, so that broker availability can be ensured even if a single node becomes unavailable through fault or for an upgrade.\nThis document describes the recommended cluster architecture and how to configure the cluster for first use, and in the future."),(0,o.kt)("h2",m({},{id:"cluster-modes"}),"Cluster Modes"),(0,o.kt)("p",null,"There are two different cluster modes available:"),(0,o.kt)("p",null,"Both modes use at least three brokers to synchronize information in case of a broker failure."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Full Sync:"),'\nThe first mode, acts as an active-passive cluster, meaning that only one of the three nodes is active and can have clients connect. The node that is active synchronizes MQTT session information and authentication information across the cluster. This means that if the active broker goes down, a client can reconnect to the new active leader exactly as if it were the original leader. This mode allows clients to communicate with one another. It is labelled "Full-Sync" in the MMC UI.'),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Dynamic-Security Sync:"),"\nThe second mode, acts as an active-active cluster, meaning that all three nodes can have clients connect to them at the same time. Just the dynamic security authentication is synchronized between nodes, meaning that clients can connect to any node and access topic as if it were any of the other nodes, but they are not able to communicate with any other given client. There is no MQTT information synchronized between brokers. This mode is useful for when clients do not communicate with each other, but send messages to a back end service that is also connected to each broker."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",m({parentName:"tr"},{align:"center"}),"Criteria\\Setup"),(0,o.kt)("th",m({parentName:"tr"},{align:"center"}),"Single Node"),(0,o.kt)("th",m({parentName:"tr"},{align:"center"}),"HA - Full Sync (active-passive)"),(0,o.kt)("th",m({parentName:"tr"},{align:"center"}),"HA - Dynamic-Security Sync (active-active)"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"All Nodes Available"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"\u2705"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"\u274c Only leader"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"\u2705")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"MQTT Session Sync"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"Not needed"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"\u2705"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"\u274c")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"Authorization & Authorization Sync"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"Not needed"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"\u2705"),(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),"\u2705")))),(0,o.kt)("h2",m({},{id:"cluster-architecture"}),"Cluster architecture"),(0,o.kt)("h3",m({},{id:"overview"}),"Overview"),(0,o.kt)("p",null,"The Mosquitto cluster comprises at least three nodes. A single node is available for use by MQTT clients at once, the other nodes operate as failover nodes. The cluster expects a minimum of two nodes to be available at once, to provide a leader and a fallback node. If the state of the cluster fails so that only a single node is available, clients will be unable to connect until the cluster is in a stable state again."),(0,o.kt)("p",null,"Figure 1 & 2 shows a suggested cluster architecture for the two available nodes. There are three broker nodes operating Mosquitto, and a fourth node providing a Management Center for Mosquitto instance (MMC)."),(0,o.kt)("img",{className:"docimagemb",src:n(94200).Z,width:"50%"}),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Figure1 Full Sync Mode")),(0,o.kt)("img",{className:"docimagemb",src:n(19296).Z,width:"50%"}),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Figure 2 Dynsec Sync Mode")),(0,o.kt)("p",null,"The nodes operate with public and private network communication. The private communication includes node-to-node synchronization data, which is not encrypted and must be kept private. In this example, all private communication happens on a separate private network in the 192.168.1.","*"," address space. If private IP addresses are not available or appropriate, then the private connection can be made using a VPN. Do not use a publicly accessible network for the cluster communication, or your credentials and data will be exposed to the internet even if your main MQTT communication is encrypted."),(0,o.kt)("p",null,"The load balancers listen on port 8883 in this example. They carry out TLS termination and forward connections. In case of the full sync mode, all connections are forwarded to the leader broker node on port 1883. While the dynsec sync mode allows forwarding to each broker."),(0,o.kt)("admonition",m({},{type:"info"}),(0,o.kt)("p",{parentName:"admonition"},"Each broker node has port 1885 open to allow connections even when the node is not the leader. This port should only be used for cluster configuration and inspection. Any other use is not covered by HA. This especially important for the full sync mode.")),(0,o.kt)("p",null,"It is strongly recommended that all public communication is encrypted. If required, using port 1883 for unencrypted connections can be done by exposing the port on the load balancer and forwarding it to the broker port 1883."),(0,o.kt)("p",null,"In this example there are three separate load balancers, and three separate broker nodes, with the broker nodes having no public networking. The purpose of the load balancer is threefold: to provide TLS/SSL termination, to route client connections to the currently available leader, and to provide separation between the public and private networks."),(0,o.kt)("p",null,"Other arrangements are possible, for example combining the load balancer and broker on nodes may be desirable in simple clusters where keeping node count low is required."),(0,o.kt)("h3",m({},{id:"docker-image-installation"}),"Docker image installation"),(0,o.kt)("p",null,"The Docker image is at ",(0,o.kt)("em",{parentName:"p"},"registry.cedalo.com/mosquitto/mosquitto:2.7"),". Cedalo Registry Credentials are needed to access this image."),(0,o.kt)("p",null,"Example docker-compose file showing the configuration described in the diagram."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{}),"version: '3.7'\n\nservices:\n   mosquitto:\n       image: registry.cedalo.com/mosquitto/mosquitto:2.7\n       ports:\n           - <private-network-ip>:1883:1883\n           - <private-network-ip>:1885:1885\n           - <private-network-ip>:7000:7000\n       volumes:\n           - ./mosquitto/config:/mosquitto/config\n           - ./mosquitto/data:/mosquitto/data\n       networks:\n           - mosquitto\n       environment:\n           CEDALO_LICENSE_FILE: /mosquitto/data/license.lic\n           CEDALO_HA_DATA_DIR: /mosquitto/data/ha\n           MOSQUITTO_DYNSEC_PASSWORD: <admin password to use when generating config>\n       restart: unless-stopped\nnetworks:\n   mosquitto:\n       name: mosquitto\n       driver: bridge\n")),(0,o.kt)("p",null,"The suggested directory layout for the volume to be mounted on the Mosquitto Docker container is:"),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{}),"/mosquitto/docker-compose.yml\n/mosquitto/mosquitto/config/mosquitto.conf\n/mosquitto/mosquitto/data/license.lic\n/mosquitto/mosquitto/data/ha/\n")),(0,o.kt)("h3",m({},{id:"broker-configuration"}),"Broker Configuration"),(0,o.kt)("p",null,"The suggested broker configuration file for each node is given below:"),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{}),"\nglobal_plugin /usr/lib/cedalo_mosquitto_ha.so\nenable_control_api true\nallow_anonymous false\n\n# The listener for receiving incoming MQTT connections from the load balancer.\n# This listener will be automatically closed when this node is not the cluster\n# leader.\n\nlistener 1883\n\n# Optional websockets listener, if used this must be exposed in the\n# docker-compose configuration\n#listener 8080\n#protocol websockets\n\n# The listener for receiving the MMC connection / API control of a follower node.\n# This listener will remain open regardless of the role of the node in the\n# cluster. It should not be used by general purpose MQTT clients and is not\n# HA supported.\nlistener 1885\nadmin_listener true\n\n# Some sensible options - tweak as per your requirements\n# Set max_keepalive to be reasonably higher than your expected max value\nmax_keepalive 1800\n# max_packet_size can protect devices against unreasonably large payloads\nmax_packet_size 100000000\n# Reduce network latency\nset_tcp_nodelay true\n")),(0,o.kt)("p",null,"In addition, environment variables must be set to configure the license file and HA data path, as demonstrated in the docker-compose example above:"),(0,o.kt)("p",null,"The MOSQUITTO_DYNSEC_PASSWORD variable is not required, but allows the initial authentication/authorisation configuration to be generated with an admin user using this password, which is convenient when generating configurations for multiple nodes. If this is not set, then each node will generate its own admin password on startup and save a plain text copy at /mosquitto/data/dynamic-security.json.pw\nThe cluster configuration is controlled through the HA MQTT topic API described below."),(0,o.kt)("h3",m({},{id:"load-balancer-configuration"}),"Load balancer configuration"),(0,o.kt)("p",null,"The load balancer should be configured to listen on the public address for the node, to terminate TLS/SSL connections, and to forward the client connections to the private addresses of the cluster nodes. In this example the broker port is 1883."),(0,o.kt)("p",null,"If the load balancer has an idle timeout value that disconnects clients if no network traffic is observed for a given period, this should be configured to be greater than the MQTT keepalive value you intend to use with your clients, otherwise there will be frequent disconnections for your idle clients. Many MQTT clients use a default of 60 seconds keepalive."),(0,o.kt)("p",null,"An example for HAProxy is given below"),(0,o.kt)("h4",m({},{id:"full-sync-ha-proxy-configuration"}),"Full Sync HA Proxy Configuration"),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{}),"global\n   user haproxy\n   group haproxy\n   daemon\n   nbproc 4\n   nbthread 1\n   maxconn 4096\n\n   # Default SSL material locations\n   ca-base /etc/ssl/certs\n   crt-base /etc/ssl/private\n   # Default ciphers to use on SSL-enabled listening sockets.\n   # For more information, see ciphers(1SSL). This list is from:\n   #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/\n   # An alternative list with additional directives can be obtained from\n   #  https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy\n   ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS\n   ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n   ssl-dh-param-file /etc/haproxy/dhparams.pem\n\ndefaults\n   log global\n   mode    tcp\n   option  dontlognull\n   timeout connect 5000\n   timeout client 2m\n   timeout server 2m\n\nfrontend mosquitto_frontend\n   bind *:8883 ssl crt /etc/haproxy/server.comb.pem\n   mode tcp\n   option tcplog\n   default_backend mosquitto_backend\n\nbackend mosquitto_backend\n   mode tcp\n   option tcplog\n   option redispatch\n   server node1 192.168.1.1:1883 check\n   server node2 192.168.1.2:1883 check\n   server node3 192.168.1.3:1883 check\n")),(0,o.kt)("h4",m({},{id:"dynsec-sync-ha-proxy-configuration"}),"Dynsec Sync HA Proxy Configuration"),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{}),"# This is node1. The config needs adapting when used on node2 and node3.\n# node1.example.com has private IP address 192.168.1.1\n# node2.example.com has private IP address 192.168.1.2\n# node3.example.com has private IP address 192.168.1.3\n\nglobal\n    log /dev/log    local0\n    user haproxy\n    group haproxy\n    daemon\n    stats socket :9999 level admin expose-fd listeners\n\n    localpeer node1.example.com\n\n    nbthread 2\n    maxconn 30000\n\n    # Default SSL material locations\n    ca-base /etc/ssl/certs\n    crt-base /etc/ssl/private\n\n    # Default ciphers to use on SSL-enabled listening sockets.\n    # For more information, see ciphers(1SSL). This list is from:\n    #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/\n    # An alternative list with additional directives can be obtained from\n    #  https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy\n    ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS\n    ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n\ndefaults\n    mode    tcp\n    option  dontlognull\n    timeout connect 5000\n    # The client and server timeout control when idle connections are\n    # closed. We have long running connections which may be idle. The\n    # longest idle time for an MQTT connection is set by the keepalive\n    # interval. If the haproxy timeouts are less than the MQTT keepalive\n    # values in use, then haproxy will disconnect clients before they get\n    # the chance to send a PINGREQ, if no other traffic is being sent.\n    # A typical keepalive is 60 seconds.\n    timeout client  70000\n    timeout server  70000\n\npeers mypeers\n    peer node1.example.com *:10001\n    peer node2.example.com 192.168.1.2:10001\n    peer node3.example.com 192.168.1.3:10001\n\nfrontend mqtt_frontend_tls\n    bind *:8883 ssl crt /usr/local/etc/haproxy/certs/server-crt-key.pem\n    default_backend mqtt_backend\n    tcp-request inspect-delay 10s\n    tcp-request content reject unless { req.payload(0,0),mqtt_is_valid }\n\nbackend mqtt_backend\n    stick-table type string len 100 size 50000 expire 24h peers mypeers\n    stick on req.payload(0,0),mqtt_field_value(connect,username)\n    option redispatch\n    server node1.example.com 192.168.1.1:1883 check on-marked-down shutdown-sessions\n    server node2.example.com 192.168.1.2:1883 check on-marked-down shutdown-sessions\n    server node3.example.com 192.168.1.3:1883 check on-marked-down shutdown-sessions\n")),(0,o.kt)("h2",m({},{id:"control-api-for-cluster"}),"Control API for Cluster"),(0,o.kt)("p",null,"The following section presents the MQTT Control API for HA. Each payload has to be published to the ",(0,o.kt)("inlineCode",{parentName:"p"},"$CONTROL/cedalo/ha/v1")," topic. To get the response of the executed command you have to subscribe to the ",(0,o.kt)("inlineCode",{parentName:"p"},"$CONTROL/cedalo/ha/v1/response")," topic."),(0,o.kt)("h3",m({},{id:"creating-the-cluster"}),"Creating the Cluster"),(0,o.kt)("p",null,"A cluster can be fully managed by the Management Center. If you choose not to use it, use the following API commands for the creation."),(0,o.kt)("h4",m({},{id:"configuring-the-first-node"}),"Configuring the first node"),(0,o.kt)("p",null,"Send the ",(0,o.kt)("em",{parentName:"p"},"createCluster")," command to the first node to initialize the cluster. The nodes array contains a list of at least three nodes to be used for the cluster. The address and port of each node are the private network address of that node and the port to be used for cluster communication. The nodeid is a unique integer for the node within the cluster, in the range 1-1023. The mynode boolean must be set to true on the details of the node where the command is being sent."),(0,o.kt)("p",null,"The command should be sent directly to the node from within the private network, rather than through the load balancer."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "createCluster",\n            "clustername": "name",\n            "nodes": [\n                { "address": "192.168.1.1", "port": 7000, "nodeid": 1, "mynode": true },\n                { "address": "192.168.1.2", "port": 7000, "nodeid": 2 },\n                { "address": "192.168.1.3", "port": 7000, "nodeid": 3 }\n            ]\n        }\n    ]\n}\n')),(0,o.kt)("p",null,"With the Cluster creation a leader is automatically elected.\nAll nodes are synced and now work via the leader authentication."),(0,o.kt)("h4",m({},{id:"joining-the-nodes-to-the-cluster"}),"Joining the nodes to the cluster"),(0,o.kt)("p",null,"Once the first cluster node is configured, the nodes in the list must be told to join the cluster. Send the ",(0,o.kt)("em",{parentName:"p"},"joinCluster")," command to each new cluster node. The ",(0,o.kt)("em",{parentName:"p"},"nodes")," list must include details of the cluster as in the ",(0,o.kt)("em",{parentName:"p"},"createCluster")," command, with the new node details having ",(0,o.kt)("em",{parentName:"p"},"mynode")," set to true. All addresses and ports are for the private network and cluster port."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "joinCluster",\n            "clustername": "name",\n            "nodes": [\n                { "address": "192.168.1.1", "port": 7000, "nodeid": 1 },\n                { "address": "192.168.1.2", "port": 7000, "nodeid": 2, "mynode": true },\n                { "address": "192.168.1.3", "port": 7000, "nodeid": 3 }\n            ]\n        }\n    ]\n}\n')),(0,o.kt)("h3",m({},{id:"adding-further-nodes"}),"Adding further nodes"),(0,o.kt)("p",null,"To add further nodes at a later date requires two steps:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Informing the existing cluster of the new node"),(0,o.kt)("li",{parentName:"ol"},"Telling the new node to join the cluster.")),(0,o.kt)("h4",m({},{id:"informing-the-cluster"}),"Informing the cluster"),(0,o.kt)("p",null,"Send the addNode command to the existing cluster node. This must be sent to the leader node. Port 1883 will only be available for the leader node, port 1885 will be available on each node, but only the leader will accept cluster commands."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "addNode",\n            "clustername": "name",\n            "address": "192.168.1.4",\n            "port": 7000,\n            "nodeid": 4\n        }\n    ]\n}\n')),(0,o.kt)("h4",m({},{id:"joining-the-node-to-the-cluster"}),"Joining the node to the cluster"),(0,o.kt)("p",null,"Send the joinCluster command to the new cluster node. The nodes list must include details of the existing cluster nodes, with mynode omitted or set to false, and the new node details with mynode set to true. All addresses and ports are for the private network and cluster port."),(0,o.kt)("admonition",m({},{type:"info"}),(0,o.kt)("p",{parentName:"admonition"},"It is strongly recommended adding nodes one at a time with the ",(0,o.kt)("em",{parentName:"p"},"addNode")," and ",(0,o.kt)("em",{parentName:"p"},"joinCluster")," commands, then verify the cluster is operating correctly before adding further nodes. If multiple nodes are added using ",(0,o.kt)("em",{parentName:"p"},"addNode")," without those nodes joining the cluster, then it is possible that the cluster will be unable to maintain consensus which would adversely affect the HA capability.")),(0,o.kt)("h3",m({},{id:"removing-nodes-from-cluster"}),"Removing nodes from Cluster"),(0,o.kt)("h3",m({},{id:"inform-cluster-leader"}),"Inform Cluster Leader"),(0,o.kt)("p",null,"To remove a node from the cluster, send the ",(0,o.kt)("em",{parentName:"p"},"removeNode")," command to the cluster leader."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "removeNode",\n            "clustername": "name",\n            "address": "192.168.1.2",\n            "port": 7000,\n            "nodeid": 2\n        }\n    ]\n}\n')),(0,o.kt)("p",null,"All details must match for the node to be removed."),(0,o.kt)("p",null,"The cluster will refuse to remove a node if it will result in the cluster having fewer than three nodes."),(0,o.kt)("h4",m({},{id:"leave-cluster"}),"Leave Cluster"),(0,o.kt)("p",null,"Once the Cluster leader is informed, remove the node from the cluster. The command should be sent to the node that is to leave the cluster, not the cluster leader. The clustername, address, port, and nodeid must match."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "leaveCluster",\n            "clustername": "name",\n            "address": "192.168.1.2",\n            "port": 7000,\n            "nodeid": 2\n        }\n    ]\n}\n')),(0,o.kt)("h3",m({},{id:"deleting-the-cluster"}),"Deleting the cluster"),(0,o.kt)("p",null,"Send the ",(0,o.kt)("em",{parentName:"p"},"deleteCluster")," command to the cluster leader. All nodes will retain their current settings, but will no longer be part of a cluster."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "deleteCluster",\n            "clustername": "name"\n        }\n    ]\n}\n')),(0,o.kt)("h3",m({},{id:"getting-cluster-information"}),"Getting cluster information"),(0,o.kt)("p",null,"Send the ",(0,o.kt)("em",{parentName:"p"},"getCluster")," command to the cluster leader."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "getCluster"\n        }\n    ]\n}\n')),(0,o.kt)("h3",m({},{id:"getting-node-information"}),"Getting node information"),(0,o.kt)("p",null,"Send the getNode command to any individual node. Nodes are always available for communication on port 1885."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "getNode"\n        }\n    ]\n}\n')),(0,o.kt)("h3",m({},{id:"set-the-cluster-leader"}),"Set the cluster leader"),(0,o.kt)("p",null,"Send the setLeader command to promote the specified node to be the cluster leader."),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "commands": [\n        {\n            "command": "setLeader",\n            "nodeid": 4\n        }\n    ]\n}\n')),(0,o.kt)("h1",m({},{id:"cluster-monitoring-documentation"}),"Cluster Monitoring Documentation"),(0,o.kt)("p",null,"Effective cluster monitoring is pivotal for maintaining the health, performance, and reliability of your system. This guide introduces three methodologies for monitoring your cluster's state, utilizing system topics, the Metrics Plugin, and the Audit Trail Plugin. Each method provides unique insights and operational benefits."),(0,o.kt)("h2",m({},{id:"overview-of-monitoring-methods"}),"Overview of Monitoring Methods"),(0,o.kt)("h3",m({},{id:"1-system-topics"}),"1. ",(0,o.kt)("strong",{parentName:"h3"},"System Topics")),(0,o.kt)("p",null,"System topics offer real-time insights into the cluster's state, facilitating immediate recognition and resolution of potential issues."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Total Nodes"),": ",(0,o.kt)("inlineCode",{parentName:"li"},'"$SYS/broker/cedalo/ha/voting_nodes/total"')," reflects the total count of nodes within the cluster, essential for understanding its overall capacity."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Online Nodes"),": ",(0,o.kt)("inlineCode",{parentName:"li"},'"$SYS/broker/cedalo/ha/voting_nodes/online"')," indicates the number of currently active nodes. Discrepancies between this figure and the total node count can highlight issues within the cluster. ",(0,o.kt)("inlineCode",{parentName:"li"},"-1")," indicates that the broker is a follower.")),(0,o.kt)("p",null,"Monitoring these topics is critical for maintaining cluster integrity and performance."),(0,o.kt)("h3",m({},{id:"2-metrics-plugin"}),"2. ",(0,o.kt)("strong",{parentName:"h3"},"Metrics Plugin")),(0,o.kt)("p",null,"The ","[Metrics Plugin for Pro Mosquitto]","(Mosquitto Manual/metrics-exporter.md) parallels the system topics' functionality, offering an integrated approach to monitoring through:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Cluster Capacity"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"mosquitto_ha_voting_nodes")," provides a count of the total nodes, akin to the information from the ",(0,o.kt)("inlineCode",{parentName:"li"},"$SYS/broker/cedalo/ha/voting_nodes/total")," topic."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Active Nodes"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"mosquitto_ha_voting_nodes_online")," tracks the number of online nodes, similar to the ",(0,o.kt)("inlineCode",{parentName:"li"},"$SYS/broker/cedalo/ha/voting_nodes/online")," topic.")),(0,o.kt)("p",null,"This plugin enhances operational oversight by facilitating the integration of cluster state metrics into broader monitoring systems."),(0,o.kt)("h3",m({},{id:"3-audit-trail-plugin"}),"3. ",(0,o.kt)("strong",{parentName:"h3"},"Audit Trail Plugin")),(0,o.kt)("p",null,"Expanding upon the capabilities of the Metrics Plugin, the ","[Audit Trail Plugin for Pro Mosquitto]","(Mosquitto Manual/audit-trail.md) offers advanced monitoring through event-driven insights and comprehensive logging:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Event-Driven Monitoring"),": Configure alerts and automations based on specific cluster state changes to improve response times and operational efficiency."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Audit Trails"),": Generate detailed logs for state changes within the cluster, aiding in troubleshooting and historical analysis.")),(0,o.kt)("p",null,'Ensure the inclusion of the "ha" module in your ',(0,o.kt)("inlineCode",{parentName:"p"},"audit-trail.json")," configuration file's filter section to capture all relevant cluster health and state change events."),(0,o.kt)("p",null,"Possible Cluster States:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"stable: The cluster has sufficient voting nodes in contact with the leader to be able to suffer the loss of at least one node without issue."),(0,o.kt)("li",{parentName:"ul"},"degraded: The cluster has the minimum number of voting nodes in contact with the leader, and hence is at risk of going offline if a single node disconnects."),(0,o.kt)("li",{parentName:"ul"},"offline: The cluster does not have a majority and is offline.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Example Log:")),(0,o.kt)("pre",null,(0,o.kt)("code",m({parentName:"pre"},{className:"language-json"}),'{\n    "timestamp": "2024-03-13T13:33:46.036Z",\n    "hostname": "mosquitto",\n    "source": "ha",\n    "operation": "clusterStatus",\n    "params": { "status": "degraded" },\n    "operationType": "update"\n}\n')),(0,o.kt)("hr",null),(0,o.kt)("p",null,"Leveraging these three methods provides a comprehensive monitoring solution, enhancing your ability to maintain a stable and reliable cluster environment."),(0,o.kt)("h2",m({},{id:"cluster-system-requirements"}),"Cluster System Requirements"),(0,o.kt)("p",null,"On each High-Availability node the components need the following system resources:"),(0,o.kt)("h3",m({},{id:"server-hardware"}),"Server hardware"),(0,o.kt)("p",null,"For the Load Balancer we recommend HAProxy. The recommended hardware components for the instance would be:\nRecent 4+ core CPU (AMD or Intel) and 8GB of RAM, if Load Balancers run separately, 8+ cores if load balancers run on same hosts as the Mosquitto nodes. As storage, 5 GB (R/W speed at least 500 MB/s) in a RAID is recommended."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",m({parentName:"tr"},{align:"center"})),(0,o.kt)("th",m({parentName:"tr"},{align:null}),(0,o.kt)("strong",{parentName:"th"},"Mosquitto Node")),(0,o.kt)("th",m({parentName:"tr"},{align:null}),(0,o.kt)("strong",{parentName:"th"},"Load balancer")),(0,o.kt)("th",m({parentName:"tr"},{align:null}),(0,o.kt)("strong",{parentName:"th"},"Mosquitto Node + Loadbalancer")))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),(0,o.kt)("strong",{parentName:"td"},"CPU:")),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"4+ Cores"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"4+ Cores"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"8+ Cores")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),(0,o.kt)("strong",{parentName:"td"},"RAM:")),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"8GB"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"8GB"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"8GB")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),(0,o.kt)("strong",{parentName:"td"},"Storage:")),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"5GB (500Mb/s +)"),(0,o.kt)("td",m({parentName:"tr"},{align:null})),(0,o.kt)("td",m({parentName:"tr"},{align:null}))))),(0,o.kt)("p",null,"In case stream processing is used, then the following recommendations apply for each node.\nWhile the CPU recommendations stay the same, it is proposed to use 24 GB of RAM and 50 GB (R/W speed at least 500 MB/s) of storage."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Stream Plugin usage:")),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",m({parentName:"tr"},{align:"center"})),(0,o.kt)("th",m({parentName:"tr"},{align:null}),(0,o.kt)("strong",{parentName:"th"},"Mosquitto Node")),(0,o.kt)("th",m({parentName:"tr"},{align:null}),(0,o.kt)("strong",{parentName:"th"},"Load balancer")),(0,o.kt)("th",m({parentName:"tr"},{align:null}),(0,o.kt)("strong",{parentName:"th"},"Mosquitto Node + Loadbalancer")))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),(0,o.kt)("strong",{parentName:"td"},"CPU:")),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"4+ Cores"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"4+ Cores"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"8+ Cores")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),(0,o.kt)("strong",{parentName:"td"},"RAM:")),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"24GB"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"24GB"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"24GB")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",m({parentName:"tr"},{align:"center"}),(0,o.kt)("strong",{parentName:"td"},"Storage:")),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"50GB (500Mb/s +)"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"50GB (500Mb/s +)"),(0,o.kt)("td",m({parentName:"tr"},{align:null}),"50GB (500Mb/s +)")))),(0,o.kt)("h3",m({},{id:"network"}),"Network"),(0,o.kt)("p",null,"As shown in the cluster architecture, the following network system is required."),(0,o.kt)("p",null,"All nodes need to have access to a public network with public IPs (where MQTT clients and browser to view MMC information reside).\nA private network with private IPs (for communication between nodes, load balancers, and between nodes and Management Center). If this is not explicitly possible then a VPN can be pulled up to privately connect the components. Private network communication is done via TCP, while the Load Balancer are used for SSL termination."),(0,o.kt)("p",null,"All nodes are 100% replicas from each other and get synchronized in real-time via the private network."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Network speed:")," Depending on expected bandwidth, 100 Mbit or 1 Gbit network is recommended."),(0,o.kt)("p",null,"The Network speeds always need to be aligned with desired message bandwidth from your MQTT clients. The upper limit for bandwidth that the Mosquitto cluster can cope with is governed by a lot of factors, however 30 MB/sec or 240 Mbit is an ultimate upper limit what a Mosquitto node can cover under certain circumstances. Therefore, it is recommended that the network between the nodes is at least a Gbit network."))}b.isMDXComponent=!0},49044:(e,t,n)=>{n.d(t,{Z:()=>l});var o=n(67294),a=n(39082),r=n(62659),i=n(83166);function l({inline:e}){return o.createElement(a.Z,{sx:{margin:e?"0px 5px":"5px 5px 15px 5px",height:e?"22px":void 0,color:"#ffffff",backgroundColor:r.Z[500]},size:"small",icon:o.createElement(i.Z,{style:{color:"white"},size:"small"}),label:"Premium"})}},19296:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/ha-dynsec-05a385ef5d8b4d065f3da4316d7eefaf.png"},94200:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/hamap-60b6711078659a8cc4bebb8b3ba79a8a.png"}}]);