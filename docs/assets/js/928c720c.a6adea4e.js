"use strict";(self.webpackChunkstreamsheets=self.webpackChunkstreamsheets||[]).push([[38349],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>d});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=p(n),d=o,h=m["".concat(s,".").concat(d)]||m[d]||c[d]||i;return n?a.createElement(h,r(r({ref:t},u),{},{components:n})):a.createElement(h,r({ref:t},u))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},99263:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>f,contentTitle:()=>d,default:()=>y,frontMatter:()=>m,metadata:()=>h,toc:()=>k});var a=n(3905),o=Object.defineProperty,i=Object.defineProperties,r=Object.getOwnPropertyDescriptors,l=Object.getOwnPropertySymbols,s=Object.prototype.hasOwnProperty,p=Object.prototype.propertyIsEnumerable,u=(e,t,n)=>t in e?o(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,c=(e,t)=>{for(var n in t||(t={}))s.call(t,n)&&u(e,n,t[n]);if(l)for(var n of l(t))p.call(t,n)&&u(e,n,t[n]);return e};const m={id:"high-availability-openshift",title:"Openshift High Availability",sidebar_label:"High Availability"},d=void 0,h={unversionedId:"deployment/on-premises/deployment/installation/openshift/high-availability-openshift",id:"version-3.0/deployment/on-premises/deployment/installation/openshift/high-availability-openshift",title:"Openshift High Availability",description:"This setup will deploy HA Mosquitto broker and Platform portal on a managed openshift cluster using Helm charts. To deploy this setup, you'll first need a  running Openshift cluster. This setup has been tested on Open source Openshift OKD cluster. Openshift offers lot of different features on top of Kubernetes. For details on Openshift and OKD you can refer to the Introduction page. For deploying a full fledged OKD cluster,  you can follow the official Openshift OKD installation documentation.  OKD can be mainly installed in two different fashion:",source:"@site/mosquitto_versioned_docs/version-3.0/deployment/on-premises/deployment/installation/openshift/high-availability-openshift.md",sourceDirName:"deployment/on-premises/deployment/installation/openshift",slug:"/deployment/on-premises/deployment/installation/openshift/high-availability-openshift",permalink:"/mosquitto/3.0/deployment/on-premises/deployment/installation/openshift/high-availability-openshift",draft:!1,tags:[],version:"3.0",frontMatter:{id:"high-availability-openshift",title:"Openshift High Availability",sidebar_label:"High Availability"},sidebar:"someSidebar",previous:{title:"Single Node",permalink:"/mosquitto/3.0/deployment/on-premises/deployment/installation/openshift/single-node-openshift"},next:{title:"High Availability Autoscaling",permalink:"/mosquitto/3.0/deployment/on-premises/deployment/installation/openshift/high-availability-autoscaling-openshift"}},f={},k=[{value:"Openshift Cluster Setup and Configuration",id:"openshift-cluster-setup-and-configuration",level:2},{value:"Dependencies and Prerequisites",id:"dependencies-and-prerequisites",level:3},{value:"Installation",id:"installation",level:2},{value:"Installation using helm charts",id:"installation-using-helm-charts",level:2},{value:"Further possible Configurations",id:"further-possible-configurations",level:4},{value:"Connect to cluster",id:"connect-to-cluster",level:2}],g={toc:k};function y(e){var t,o=e,{components:u}=o,m=((e,t)=>{var n={};for(var a in e)s.call(e,a)&&t.indexOf(a)<0&&(n[a]=e[a]);if(null!=e&&l)for(var a of l(e))t.indexOf(a)<0&&p.call(e,a)&&(n[a]=e[a]);return n})(o,["components"]);return(0,a.kt)("wrapper",(t=c(c({},g),m),i(t,r({components:u,mdxType:"MDXLayout"}))),(0,a.kt)("p",null,"This setup will deploy HA Mosquitto broker and Platform portal on a managed openshift cluster using Helm charts. To deploy this setup, you'll first need a  running Openshift cluster. This setup has been tested on Open source Openshift OKD cluster. Openshift offers lot of different features on top of Kubernetes. For details on Openshift and OKD you can refer to the Introduction page. For deploying a full fledged OKD cluster,  you can follow the official Openshift OKD installation ",(0,a.kt)("a",c({parentName:"p"},{href:"https://docs.okd.io/"}),"documentation"),".  OKD can be mainly installed in two different fashion:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"IPI: Installer Provisioned Infrastructure"),(0,a.kt)("li",{parentName:"ol"},"UPI: User Provisioned Infrastructure")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Installer Provisioned Infrastructure:"),"  Installer Provisioned Infrastructure (IPI) in OKD/OpenShift refers to a deployment model where the installation program provisions and manages all the components of the infrastructure needed to run the OpenShift cluster. This includes the creation of virtual machines, networking rules, load balancers, and storage components, among others. The installer uses cloud-specific APIs to automatically set up the infrastructure, making the process faster, more standardized, and less prone to human error compared to manually setting up the environment."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"User Provisioned Infrastructure:"),"  User Provisioned Infrastructure (UPI) in OKD/OpenShift is a deployment model where users manually create and manage all the infrastructure components required to run the OpenShift cluster. This includes setting up virtual machines or physical servers, configuring networking, load balancers, storage, and any other necessary infrastructure components. Unlike the Installer Provisioned Infrastructure (IPI) model, where the installation program automatically creates and configures the infrastructure, UPI offers users complete control over the deployment process."),(0,a.kt)("p",null,"You are free to choose your own method among the two. You can also choose the cloud provider you want to deploy your solution on. Openshift OKD supports number of different cloud providers and also gives you an option to do bare-metal installation. In this deployment we went forward with UPI and deployed our infrastructure on Google Cloud Platform (GCP) using the ",(0,a.kt)("inlineCode",{parentName:"p"},"Private cluster")," method mentioned ",(0,a.kt)("a",c({parentName:"p"},{href:"https://docs.okd.io/latest/installing/installing_gcp/installing-gcp-private.html"}),"here"),". Therefore, this solution is developed and tested on GCP, however it is unlikely that basic infrastructure would differ across different cloud providers."),(0,a.kt)("p",null,"A ",(0,a.kt)("inlineCode",{parentName:"p"},"private cluster")," in GCP ensures that the nodes are isolated in a private network, reducing exposure to the public internet but again you are free to choose your own version of infrastructure supported by Openshift OKD.  We will briefly discuss how the infrastructure looks like in our case so that you can have a reference for your own infrastructure."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"OKD Infrastructure (During provisioning)",src:n(31070).Z,title:"OKD Infrastructure on GCP during provisioning",width:"751",height:"510"})),(0,a.kt)("p",null,"Figure 1: OKD Infrastructure on GCP during provisioning"),(0,a.kt)("p",null,"The diagram depicts the deployment process for our OCP cluster on GCP, starting with the establishment of a bastion host. Bastion host is where we'll execute commands to configure the bootstrap node, then the Master nodes, and finally, the worker nodes in a separate subnet. Before initiating the bootstrap procedure, we set up the essential infrastructure components, including networks, subnetworks, an IAM service account, an IAM project, a Private DNS zone, Load balancers, Cloud NATs, and a Cloud Router."),(0,a.kt)("p",null,"Upon completing the bootstrap phase, we dismantled the bootstrap components from the cluster. Subsequently, we focussed on creating the worker nodes. After the worker nodes are operational, we set up a reverse proxy on the bastion host to facilitate local access to the OCP Console UI through our browser. To conclude, we confirm that all cluster operators are marked as \u2018Available\u2019. Once we done with the provisioning the architecture would look something like Figure 2. More detailed steps can be found in the official documentation.  These discussed steps are all part of the official  ",(0,a.kt)("a",c({parentName:"p"},{href:"https://docs.okd.io/latest/installing/installing_gcp/installing-gcp-private.html"}),"documentation"),"."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"OKD Infrastructure (Post provisioning)",src:n(87083).Z,title:"OKD Infrastructure on GCP post provisioning ",width:"684",height:"676"})),(0,a.kt)("p",null,"Figure 2: OKD Infrastructure on GCP post provisioning"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"Note:")," This deployment involves setting up a private cluster, which means access to the cluster is limited to through the bastion host. Consequently, we avoid using public DNS for this installation, relying solely on a private DNS zone. To facilitate access to the external UI, we will employ a reverse proxy for this purpose."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"HA-PROXY Configurations"),"\nHA-proxy need to be configured accordingly for the kubernetes setup. For server m1, m2 and m3 needs to be configured in this case. Instead of using docker IP we would use DNS names to address the pods. For eg\n",(0,a.kt)("inlineCode",{parentName:"p"},"mosquitto-0.mosquitto.ha.svc.cluster.local"),". Here ",(0,a.kt)("inlineCode",{parentName:"p"},"mosquitto-0"),",",(0,a.kt)("inlineCode",{parentName:"p"},"mosquitto-1"),",",(0,a.kt)("inlineCode",{parentName:"p"},"mosquitto-2")," are the name of individual mosquitto pods running as statefulsets. Each new pod would increase its pod-ordinal by 1. Rest can be defined as follows\n",(0,a.kt)("inlineCode",{parentName:"p"},"<pod-name>.<name-of-the-statefulset>.<namespace>.svc.cluster.local")),(0,a.kt)("p",null,"Your setup folder comes along with a default configuration of haproxy config which is given below. Make sure you pass the correct nameserver ip during installation so that haproxy can keep track of mosquitto pods upgradation and updates. ."),(0,a.kt)("pre",null,(0,a.kt)("code",c({parentName:"pre"},{}),"global\n    daemon\n    maxconn 10000\n\nresolvers kube-dns\n    nameserver dns {{ .Values.nameserver }}:53  # Replace with your actual kube-dns ClusterIP if different\n    hold valid 10s\n    timeout resolve 5s\n    timeout retry   1s\n\nfrontend mqtt_frontend\n    bind *:{{ .Values.haproxy.ports.listener }}\n    mode tcp\n    maxconn 10000\n    default_backend mqtt_backend\n    timeout client 10m\n\nbackend mqtt_backend\n    timeout connect 5s\n    timeout server 10m\n    mode tcp\n    option redispatch\n    server m1 mosquitto-0.mosquitto.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.mosquitto.ports.listenerTarget }} resolvers kube-dns resolve-prefer ipv4 check inter 5s fall 3 rise 2 on-marked-down shutdown-sessions\n    server m2 mosquitto-1.mosquitto.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.mosquitto.ports.listenerTarget }} resolvers kube-dns resolve-prefer ipv4 check inter 5s fall 3 rise 2 on-marked-down shutdown-sessions\n    server m3 mosquitto-2.mosquitto.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.mosquitto.ports.listenerTarget }} resolvers kube-dns resolve-prefer ipv4 check inter 5s fall 3 rise 2 on-marked-down shutdown-sessions\n")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"Note"),": Make sure to add more entries to this haproxy-config.yaml  file if you wish to add more mosquitto nodes to this cluster. You would have to reconfigure the haproxy-config.yaml in that case."),(0,a.kt)("h2",c({},{id:"openshift-cluster-setup-and-configuration"}),"Openshift Cluster Setup and Configuration"),(0,a.kt)("h3",c({},{id:"dependencies-and-prerequisites"}),"Dependencies and Prerequisites"),(0,a.kt)("p",null,"As we chose to use a private cluster, therefore master and worker nodes would not have access to the internet. Therefore, we will install the dependencies on the bastion node and would also deploy the application from the bastion node."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prerequisites")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Running Openshift cluster by following the official documentation guide of ",(0,a.kt)("a",c({parentName:"li"},{href:"https://docs.okd.io/"}),"Openshift"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Create a namespace")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Once you are connected to your Openshift setup and can access the cluster using ",(0,a.kt)("inlineCode",{parentName:"li"},"oc")," tool. Create a namespace in which you would want to deploy the application. The deployment folder is pre-configured for the namespace named ",(0,a.kt)("inlineCode",{parentName:"li"},"ha"),". If you want to use the default configuration you can create a namespace named ",(0,a.kt)("inlineCode",{parentName:"li"},"ha")," using the below command:"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"oc create namespace ha")),(0,a.kt)("li",{parentName:"ul"},"If you want to use a different namespace, use the command: ",(0,a.kt)("inlineCode",{parentName:"li"},"oc create namespace <your-custom-namespace>"),".  Replace ",(0,a.kt)("inlineCode",{parentName:"li"},"<your-custom-namespace>")," with the name of the namespace you want to configure.")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"On your bastion node"),": Check the allocated user id for your namespace after you already created your desired namespace (step 2). You can check the allocated user id for your namespace by running the command ",(0,a.kt)("inlineCode",{parentName:"p"},"oc describe namespace <namespace>")," where ",(0,a.kt)("inlineCode",{parentName:"p"},"<namespace>")," is the namespace you chose while creating the namespace in step 2. For default namespace i.e ",(0,a.kt)("inlineCode",{parentName:"p"},"ha"),", the command would be ",(0,a.kt)("inlineCode",{parentName:"p"},"oc describe namespace multinode"),"."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"The above command would output a response.  A sample output could be like:"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",c({parentName:"pre"},{}),"      Name:         ha\n      Labels:       kubernetes.io/metadata.name=multinode\n                    pod-security.kubernetes.io/audit=restricted\n                    pod-security.kubernetes.io/audit-version=v1.24\n                    pod-security.kubernetes.io/warn=restricted\n                    pod-security.kubernetes.io/warn-version=v1.24\n      Annotations:  openshift.io/sa.scc.mcs: s0:c27,c4\n                    openshift.io/sa.scc.supplemental-groups: 1000710000/10000\n                    openshift.io/sa.scc.uid-range: 1000710000/10000\n      Status:       Active\n\n      No resource quota.\n      No LimitRange resource.\n"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Note down the value for ",(0,a.kt)("inlineCode",{parentName:"p"},"openshift.io/sa.scc.uid-range"),". The noted user id will now be used to install the helm chart. This user id needs to be propagated to the pods so that they could have adequate permissions while execution without needing additional security policy. After checking the environment prerequisites are set, follow we will prepare the Mosquitto environment:"))),(0,a.kt)("h2",c({},{id:"installation"}),"Installation"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prerequisites"),":"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Openshift  Cluster should be up and running."),(0,a.kt)("li",{parentName:"ol"},"You have successfully created the namespace and noted down the uid range.")),(0,a.kt)("h2",c({},{id:"installation-using-helm-charts"}),"Installation using helm charts"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Get the helm chart")),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Make sure you have the helm chart  ",(0,a.kt)("inlineCode",{parentName:"li"},"mosquitto-3.0-platform-3.0-openshift-cluster.tgz")," downloaded from the Platform portal. In this case, we have the helm chart on our Bastion node. It could differ based on the infrastructure."))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Install Helm Chart:")),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Use the following ",(0,a.kt)("inlineCode",{parentName:"li"},"helm install")," command to deploy the Mosquitto application on to your Openshift cluster. Replace ",(0,a.kt)("inlineCode",{parentName:"li"},"<release-name>")," with the desired name for your Helm release and ",(0,a.kt)("inlineCode",{parentName:"li"},"<namespace>")," with your chosen Kubernetes namespace:",(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",c({parentName:"pre"},{className:"language-bash"}),"helm install <release-name>  mosquitto-3.0-platform-3.0-openshift-cluster.tgz  -n <namespace> --set runAsUser=<namespace-alloted-user-id> --set nameserver=<nameserver-ip>\n"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"namespace"),": Set it to the namespace of your deployment."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"namespace-alloted-user-id"),": Set it to user id you noted through ",(0,a.kt)("inlineCode",{parentName:"li"},"oc describe namespace singlenode")," command."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"nameserver-ip"),": Set it to kube-dns ClusterIP. Generally you can find the IP using ",(0,a.kt)("inlineCode",{parentName:"li"},"kubectl get svc -n kube-system")),(0,a.kt)("li",{parentName:"ul"},"Further useful flags (use it based on your requirements):",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--set storageClassName=<storage-class-name>"),": Assign your custom storage class name based on your cloud provider. Default is set ",(0,a.kt)("inlineCode",{parentName:"li"},"standard-rwo")," (from GCP)."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--set mosquitto.serviceType=<serviceType>"),":  Options available are ",(0,a.kt)("inlineCode",{parentName:"li"},"LoadBalancer"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"NodePort")," or ",(0,a.kt)("inlineCode",{parentName:"li"},"ClusterIP"),". Default is set to ",(0,a.kt)("inlineCode",{parentName:"li"},"LoadBalancer"),". If you do not want ",(0,a.kt)("inlineCode",{parentName:"li"},"LoadBalancer")," as a service type but still want external access, you can deploy using either ",(0,a.kt)("inlineCode",{parentName:"li"},"NodePort")," or ",(0,a.kt)("inlineCode",{parentName:"li"},"ClusterIP")," and then configure ingress controller on top of it. Ingress does not come along with helm charts."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--set platform.serviceType=<serviceType>"),":  Options available are ",(0,a.kt)("inlineCode",{parentName:"li"},"LoadBalancer"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"NodePort")," or ",(0,a.kt)("inlineCode",{parentName:"li"},"ClusterIP"),". Default is set to ",(0,a.kt)("inlineCode",{parentName:"li"},"LoadBalancer"),". If you do not want ",(0,a.kt)("inlineCode",{parentName:"li"},"LoadBalancer")," as a service type and also want external access, you can deploy using either ",(0,a.kt)("inlineCode",{parentName:"li"},"NodePort")," or ",(0,a.kt)("inlineCode",{parentName:"li"},"ClusterIP")," and then configure ingress controller on top of it. Ingress does not come along with helm charts."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--set mosquitto.pullPolicy=<pullPolicy>"),":  Assign based on your preference. Default is set to ",(0,a.kt)("inlineCode",{parentName:"li"},"Always"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--set mosquitto.replicas=<replica-count>"),":  The max limit of the number of replicas is limited by your license."))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Note:")," You can explore further configuration in ",(0,a.kt)("inlineCode",{parentName:"li"},"values.yaml")),(0,a.kt)("li",{parentName:"ul"},"Sample example: If  your arbitrary release name is ",(0,a.kt)("inlineCode",{parentName:"li"},"sample-release-name"),", your user id for your namespace is ",(0,a.kt)("inlineCode",{parentName:"li"},"1000710000")," then your helm installation command should be:",(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",c({parentName:"pre"},{className:"language-bash"}),"helm install sample-release-name  mosquitto-3.0-platform-3.0-openshift-cluster.tgz  -n ha --set runAsUser=1000710000 \n"))))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"You can monitor the running pods using the ",(0,a.kt)("inlineCode",{parentName:"p"},"oc get pods -o wide -n <namespace>")," command. To observe the service endpoints ports use ",(0,a.kt)("inlineCode",{parentName:"p"},"oc get svc -n <namespace>"),".")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"The default type of cluster would a HA cluster.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"To uninstall the setup:\n",(0,a.kt)("inlineCode",{parentName:"p"},"helm uninstall <release-name> -n <namespace>")))),(0,a.kt)("p",null,"Your Mosquitto setup is now running  with three single mosquitto nodes in a Dynsec cluster along with a Platform portal."),(0,a.kt)("h4",c({},{id:"further-possible-configurations"}),"Further possible Configurations"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Add Nodes"),": If you want to add more nodes to the existing cluster of three nodes. Make sure your license allows more than three nodes before proceeding. You can follow the steps to add more nodes"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Configure your HA-proxy by adding entries to the ",(0,a.kt)("inlineCode",{parentName:"p"},"haproxy-config.yaml"),".")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"p"},"haproxyconfig")," configmap is part of the compressed Helm chart. Therefore, to edit the configmap, uncompress the helm chart using the following command:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"tar -xzvf mosquitto-3.0-platform-3.0-openshift-cluster.tgz")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"cd mosquitto-3.0-platform-3.0-openshift-cluster/templates/")),(0,a.kt)("li",{parentName:"ul"},"Edit ",(0,a.kt)("inlineCode",{parentName:"li"},"haproxy-config.yaml")," by adding below mentioned line at end the the file\n",(0,a.kt)("inlineCode",{parentName:"li"},"server m4 mosquitto-3.mosquitto.<namespace>.svc.cluster.local:1888 check on-marked-down shutdown-sessions"),". Replace ",(0,a.kt)("inlineCode",{parentName:"li"},"<namespace>")," with your chosen namespace. This config line assumes you are adding the fourth node. You can add more more nodes similarly however make sure you update the server address."))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Go back to the parent directory:\n",(0,a.kt)("inlineCode",{parentName:"p"},"cd ../"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Package the helm chart to its original form using:\n",(0,a.kt)("inlineCode",{parentName:"p"},"helm package mosquitto-3.0-platform-3.0-openshift-cluster"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Uninstall the existing helm deployment, if you want to completely remove existing setup:\n",(0,a.kt)("inlineCode",{parentName:"p"},"helm uninstall <release-name> -n <namespace>"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Reinstall helm package if you used ",(0,a.kt)("inlineCode",{parentName:"p"},"helm uninstall"),", so that new configmaps can come into effect (Use the same command you used to install it the first time with appropriate flags. Below command example is just a reference)"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",c({parentName:"pre"},{className:"language-bash"})," helm install <release-name>  mosquitto-3.0-platform-3.0-openshift-cluster.tgz  -n <namespace> --set runAsUser=<namespace-alloted-user-id> \n"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"If you want to upgrade your existing setup without uninstalling it. You can also upgrade the existing setup using ",(0,a.kt)("inlineCode",{parentName:"p"},"helm upgrade"),". You can upgrade it using a similar command to ",(0,a.kt)("inlineCode",{parentName:"p"},"helm install"),"."),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",c({parentName:"pre"},{className:"language-bash"}),"  helm upgrade <release-name>  mosquitto-3.0-platform-3.0-openshift-cluster.tgz  -n <namespace> --set runAsUser=<namespace-alloted-user-id> \n"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"As you have now added the haproxy config and installed the mosquitto node. In order to scale the number of pods from the existing three mosquitto pods, you can scale the Kubernetes replica count of statefulset to desired number. For eg if you want to increase the number of nodes from default three nodes to four nodes you can use the following command:\n",(0,a.kt)("inlineCode",{parentName:"p"}," oc scale statefulsets mosquitto --replicas=4 -n <namespace>"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Adding new node to existing cluster:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Navigate to ",(0,a.kt)("inlineCode",{parentName:"li"},"Connections")," section on Platform portal and click ",(0,a.kt)("inlineCode",{parentName:"li"},"Edit"),"."),(0,a.kt)("li",{parentName:"ul"},"Click  ",(0,a.kt)("inlineCode",{parentName:"li"},"Add Node"),"."),(0,a.kt)("li",{parentName:"ul"},"Fill in the details like ",(0,a.kt)("inlineCode",{parentName:"li"},"Name")," and  ",(0,a.kt)("inlineCode",{parentName:"li"},"Private Address")," (make sure that the ",(0,a.kt)("inlineCode",{parentName:"li"},"Private Address"),"  does not conflict with the existing ones), ",(0,a.kt)("inlineCode",{parentName:"li"},"NAME"),": name of the connection (you can choose your own name)."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"URL"),": ",(0,a.kt)("inlineCode",{parentName:"li"},"mqtt://mosquitto-x.mosquitto.<namespace>.svc.cluster.local:1885"),"  . Replace ",(0,a.kt)("inlineCode",{parentName:"li"},"x")," in ",(0,a.kt)("inlineCode",{parentName:"li"},"mosquitto-x")," with a valid number based on the which node you are adding. If you are adding a fourth node and the name of your namespace is  ",(0,a.kt)("inlineCode",{parentName:"li"},"ha")," then the entry would be ",(0,a.kt)("inlineCode",{parentName:"li"},"mqtt://mosquitto-3.mosquitto.multinode.svc.cluster.local:1885"),". If you are adding a fifth node and the name of your namespace is  ",(0,a.kt)("inlineCode",{parentName:"li"},"ha")," then the entry would be ",(0,a.kt)("inlineCode",{parentName:"li"},"mqtt://mosquitto-4.mosquitto.ha.svc.cluster.local:1885"),"."),(0,a.kt)("li",{parentName:"ul"},"Add your username and password given by the cedalo team, select/deselect further options."),(0,a.kt)("li",{parentName:"ul"},"Select ",(0,a.kt)("inlineCode",{parentName:"li"},"Test Connection")," to verify your connection. If the connection succeeds proceed to next step or else verify your configuration.",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Select ",(0,a.kt)("inlineCode",{parentName:"li"},"Add Node")," and  click ",(0,a.kt)("inlineCode",{parentName:"li"},"Save"),"."))))))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Remove Nodes")),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"You can start by removing the node configuration from the existing cluster using the ",(0,a.kt)("inlineCode",{parentName:"li"},"Cluster Node")," section in the Platform portal and delete the additional node. Make sure you maintain at least 3 nodes in the cluster for HA ."),(0,a.kt)("li",{parentName:"ul"},"Then you can set the replica count to desired number. For eg if you want to decrease the number of nodes from four nodes to three nodes you can use the following command.\n",(0,a.kt)("inlineCode",{parentName:"li"}," oc  scale statefulsets mosquitto --replicas=3 -n <namespace>")))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Further Useful Commands")),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"If you want to change mosquitto.conf, you can do so by uncompressing the helm chart, making the required changes and packaging the helm charts again. The detailed procedure is mentioned below:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"tar -xzvf mosquitto-3.0-platform-3.0-openshift-cluster.tgz")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"cd mosquitto-3.0-platform-3.0-openshift-cluster/files/")),(0,a.kt)("li",{parentName:"ul"},"Make changes to ",(0,a.kt)("inlineCode",{parentName:"li"},"mosquitto.conf")," and save it."),(0,a.kt)("li",{parentName:"ul"},"Go back to the parent directory:\n",(0,a.kt)("inlineCode",{parentName:"li"},"cd ../")),(0,a.kt)("li",{parentName:"ul"},"Package the helm chart to its original form using:\n",(0,a.kt)("inlineCode",{parentName:"li"},"helm package mosquitto-3.0-platform-3.0-openshift-cluster")),(0,a.kt)("li",{parentName:"ul"},"Uninstall helm package\n",(0,a.kt)("inlineCode",{parentName:"li"},"helm uninstall <release-name> -n <namespace>")),(0,a.kt)("li",{parentName:"ul"},"Reinstall the helm package using the same ",(0,a.kt)("inlineCode",{parentName:"li"},"helm install")," command you used the first time or do a ",(0,a.kt)("inlineCode",{parentName:"li"},"helm upgrade"),"."))),(0,a.kt)("li",{parentName:"ul"},"You can similarly change any other configuration like ",(0,a.kt)("inlineCode",{parentName:"li"},"sql-bridge")," configuration, repackage the helm chart and install to bring changes into effect.")))),(0,a.kt)("h2",c({},{id:"connect-to-cluster"}),"Connect to cluster"),(0,a.kt)("p",null,"Once your setup is ready, you can access the mosquitto brokers using the external ip of the haproxy service deployed to connect to the cluster from the outside world.\nGo to the ",(0,a.kt)("inlineCode",{parentName:"p"},"Client Account"),' menu and create a new client to connect from. Make sure to assign a role, like the default "client" role, to allow your client to publish and/or subscribe to topics.\nNow you can connect to the Mosquitto cluster. You can access it either with connecting it directly to worker node running the haproxy pod along with a service exposed at port 1883:'),(0,a.kt)("p",null,"To get the external ip of HAproxy service:\n",(0,a.kt)("inlineCode",{parentName:"p"},"kubectl get service haproxy -n <namespace>")),(0,a.kt)("p",null,"In this example command we use Mosquitto Sub to subscribe onto all topics:\n",(0,a.kt)("inlineCode",{parentName:"p"},"mosquitto_sub -h <external-ip-of-haproxy> -p 1883 -u <username> -P <password> -t '#'")),(0,a.kt)("p",null,"Make sure to replace your IP, username and password."))}y.isMDXComponent=!0},31070:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/okd-infrastructure-1-bba8c29af6e08db7c6f3e61d79bfd615.png"},87083:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/okd-infrastructure-2-abe5236cfef5147cd3d552b6886cc976.png"}}]);